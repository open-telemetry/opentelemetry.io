---
title: Improved troubleshooting using k8s attributes
linkTitle: Kubernetes attributes
date: 2022-06-23
author: Ruben Vargas
spelling: cSpell:ignore k8sattributes K8sattributes K8sprocessor KUBE
---

Attaching kubernetes resource attributes to traces is something can be very
useful because it lets you identify which resource (such as a pod) is failing or having
performance problems. It is also generally useful for correlating across other signals.

In this article you will learn how to configure the OpenTelemetry collector to
use the k8sattributes processor, and we will explore how to configure it in
different scenarios. At the end, we present some alternatives and possible
future improvements.

Details of the OpenTelemetry collector pipeline won't be covered in this post. For those details
please, refer to [OpenTelemetry collector documentation](/docs/collector/).

## How k8s attributes are attached

At a high level, k8s attributes are attached to traces as [Resources](/docs/concepts/glossary/#resource). This is for two reasons:

 1. They fit the definition of what a Resource is - an entity for which telemetry is recorded
 2. Centralizes this metadata, which is relevant for any span generated

Let's dive in and see how to do it!

## Using k8sattributes processor

The `k8sattributesprocessor` is an OpenTelemetry collector processor that attaches
Kubernetes attributes to spans, logs and metrics.

This processor automatically discovers pod attributes and attaches those
attributes to the spans generated by that pod. If the pod belongs to a
deployment or a ReplicaSet, it will also discover and attach some deployment
attributes.

Some attributes we can attach to the spans are:

- node name `k8s.node.name`.
- pod name `k8s.pod.name`.
- pod UID `k8s.pod.uid`.
- namespace `k8s.namespace.name`.
- deployment name, if the pod was created by a deployment.

Those attributes use the OpenTelemetry semantic convention. You can see a
full list
[here](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/semantic_conventions/k8s.md).

The processor internally maintains a list of pods and an associated attribute,
usually the IP address of the pod, and uses this attribute to know which pod
generates a certain span.

![k8sattributes processor data flow](/img/blog-k8sattributes/k8sprocessor.png)

In the figure above we can see how the data flows: The table of pods is fetched
using Kubernetes API, while the pod IP is extracted from the connection context
between the pod and the collector

The k8sattributes processor can work in different modes, and it depends on how
the collector is configured. We are going to explore one common scenario, when
the collector is deployed as daemonset.

### DaemonSet mode

Let’s take a look at how we can configure the collector in daemonset mode, known
as an agent mode in the k8sattributes documentation.

When we deploy the collector as the DaemonSet we have one collector pod per
node. We need to configure the collector service account to have permissions to
fetch all pods information. In order to do that, we will create a `ClusterRole`
with the necessary permissions.

Here are the minimum permissions required for make k8sattributes processor
works:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
```

Now, we are going to deploy the collector in daemonset mode. It is recommended
to set a filter to only fetch the pods that belong to the node in which the
collector is deployed. This is because if we are on a large cluster, we don’t
want to maintain a huge list of pods.

This is the manifest used in this blog to show how the processor works:

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector-daemonset
spec:
  mode: daemonset
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0
  serviceAccount: attributes-account
  env:
    - name: KUBE_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
  config: |
    receivers:
      jaeger:
          protocols:
              grpc:
              thrift_binary:
              thrift_compact:
              thrift_http:
      otlp:
          protocols:
              grpc:
              http:

    processors:
         k8sattributes:
             filter:
                 node_from_env_var: KUBE_NODE_NAME
    exporters:
      jaeger:
        endpoint: jaeger-all-in-one-collector:14250
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [k8sattributes]
          exporters: [jaeger]
```

The main parts to note are that we are using the contrib collector image, the
reason is because this processor is not part of the OpenTelemetry collector
core. Other things to notice are the filter mentioned above, and the use of a
specific service account which is created previously and contains the
permissions to fetch the pod list.

Now, we can deploy the manifest above and the vertex app example to generate
some traces.

![Jaeger UI showing the span attributes](/img/blog-k8sattributes/jaeger-k8sattributes.png)

As we can see, each span of the trace now has the corresponding pod attributes
attached to it.

We can restrict the above configuration to a certain namespace if we add the
namespace on the k8sattributes processor filter like this:

```yaml
processors:
  k8sattributes:
    filter:
      namespace: my_namespace
```

In this way we can create a `Role` and don’t need to create a `ClusterRole`,
reducing the scope of the collector service account to a single namespace.

## Using Resource detector processor

Recently the OpenTelemetry operator incorporated a new feature introduced in
version v0.50.0, see
[PR #832](https://github.com/open-telemetry/opentelemetry-operator/pull/832).

This feature sets the `OTEL_RESOURCE_ATTRIBUTES` environment variable on the
collector container with the k8s pod attributes. It allows us to use the
resource detector processor, which attaches the environment variable values to
the spans. This only works when the collector is deployed in sidecar mode.

For example if we deploy this manifest:

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: sidecar-for-my-app
spec:
  mode: sidecar
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0
  config: |
    receivers:
      jaeger:
        protocols:
          grpc:
          thrift_binary:
          thrift_compact:
          thrift_http:
      otlp:
        protocols:
          grpc:
          http:

    processors:
      resourcedetection:
        detectors: [env]
        timeout: 2s
        override: false

    exporters:
      jaeger:
        endpoint: jaeger-all-in-one-collector:14250
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [resourcedetection]
          exporters: [jaeger]
```

And then we deploy the vertx app example, we can see in the sidecar container,
the `OTEL_RESOURCE_ATTRIBUTES` was injected with some values, some of them are
using the Kubernetes downward API to get the attribute values.

An example of the value of the env var:

```yaml
- name: OTEL_RESOURCE_ATTRIBUTES
  value: k8s.deployment.name=dep-vertex,k8s.deployment.uid=ef3fe26b-a690-4746-9119-d2dbd94b469f,
  k8s.namespace.name=default,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=
  (OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.pod.uid=$(OTEL_RESOURCE_ATTRIBUTES_POD_UID),k8s.replicaset
  name=dep-vertex-59b6f76585,k8s.replicaset.uid=5127bc38-e298-40e1-95df-f4a777e3176c
```

## Conclusions

We have learned about how to do a basic configuration that allows the
OpenTelemetry collector to attach Kubernetes resource attributes to traces.
There could be other complex scenarios but I hope this could serve as a base
that allows the reader to understand how the to add Kubernetes attributes to the
spans generated by the pods. We explored how to configure the k8sattributes
operator in order to accomplish this, and saw some improvements introduced on
the OpenTelemetry operator that allow us to do this in a simple way on sidecar
mode

## References

- [K8sattributes processor](https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/processor/k8sattributesprocessor)
- [K8sattributes processor RBAC](https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/processor/k8sattributesprocessor#hdr-RBAC)
- [OpenTelemetry Kubernetes attributes](/docs/reference/specification/resource/semantic_conventions/k8s)
