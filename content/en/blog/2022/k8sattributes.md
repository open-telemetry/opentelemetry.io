---
title: Improved troubleshooting using k8s attributes
linkTitle: Kubernetes attributes
date: 2022-06-23
author: Ruben Vargas
spelling: cSpell:ignore k8sattributes K8sattributes K8sprocessor KUBE
---

Attaching kubernetes resource attributes to traces is something can be very
useful because it lets you identify which resource (such as a pod) is failing or
having performance problems. It is also generally useful for correlating across
other signals.

In this article, you'll learn how to configure the OpenTelemetry collector to
use the `k8sattributesprocessor` processor, and we will explore how to configure
it in different scenarios. At the end, we present some alternatives.

Details of the OpenTelemetry collector pipeline won't be covered in this post.
For those details please, refer to
[OpenTelemetry collector documentation](/docs/collector/).

## How k8s attributes are attached

At a high level, k8s attributes are attached to traces as
[Resources](/docs/concepts/glossary/#resource). This is for two reasons:

1.  They fit the definition of what a Resource is - an entity for which
    telemetry is recorded
2.  Centralizes this metadata, which is relevant for any span generated

Let's dive in and see how to do it!

## Using k8sattributes processor

The `k8sattributesprocessor` is an OpenTelemetry collector processor that
attaches Kubernetes attributes to spans, logs and metrics.

This processor automatically discovers pod attributes and attaches those
attributes to resource asociated with the spans generated by that pod. If the
pod belongs to a `Deployment` or a `ReplicaSet`, it will also discover it's
attributes.

Some attributes we can attach to the resource are:

- node name `k8s.node.name`.
- pod name `k8s.pod.name`.
- pod UID `k8s.pod.uid`.
- namespace `k8s.namespace.name`.
- deployment name, if the pod was created by a deployment.

Those attributes use the OpenTelemetry semantic convention. You can see a full
list
[here](https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/semantic_conventions/k8s.md).

The processor internally maintains a list of pods and an associated attribute,
usually the IP address of the pod, and uses this attribute to know which pod
generates a certain span.

![k8sattributes processor data flow](/img/blog-k8sattributes/k8sprocessor.png)

In the figure above you can see how the data flows: The table of pods is fetched
using Kubernetes API, while the pod IP is extracted from the connection context
between the pod and the collector.

The `k8sattributesprocessor` can work in different modes, and it depends on how
the collector is configured. Let's explore one common scenario, when the
collector is deployed as daemonset.

### DaemonSet mode

Let’s take a look at how we can configure the collector in daemonset mode, known
as an agent mode in the k8sattributes documentation.

When we deploy the collector as the DaemonSet we have one collector pod per
node. We need to configure the collector service account to have permissions to
fetch all pods information. In order to do that, we will create a `ClusterRole`
with the necessary permissions.

Here are the minimum permissions required to make the `k8sattributesprocessor`
work:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
```

Next, deploy the collector in daemonset mode. It is recommended
to set a filter to only fetch the pods that belong to the node in which the
collector is deployed. This is because if you have a large cluster, you don’t
want to maintain a huge list of pods.

This is the manifest used in this blog to show how the processor works:

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector-daemonset
spec:
  mode: daemonset
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0
  serviceAccount: attributes-account
  env:
    - name: KUBE_NODE_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: spec.nodeName
  config: |
    receivers:
      jaeger:
          protocols:
              grpc:
              thrift_binary:
              thrift_compact:
              thrift_http:
      otlp:
          protocols:
              grpc:
              http:

    processors:
         k8sattributes:
             filter:
                 node_from_env_var: KUBE_NODE_NAME
    exporters:
      jaeger:
        endpoint: jaeger-all-in-one-collector:14250
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [k8sattributes]
          exporters: [jaeger]
```

The main parts to note are that it uses the contrib collector image. The
`k8sattributesprocessor` is not part of the OpenTelemetry collector
core, but the contrib distribution has it. Other things to notice are the filter
mentioned above, and the use of a previously-created specific service account,
which contains the permissions to fetch the pod list.

Next, deploy the manifest and the vertex app example to generate
some traces.

![Jaeger UI showing the span attributes](/img/blog-k8sattributes/jaeger-k8sattributes.png)

As we can see, each span of the trace now has the corresponding pod attributes
attached to it.

You can restrict the above configuration to a certain namespace if you add the
namespace on the `k8sattributesprocessor` filter like this:

```yaml
processors:
  k8sattributes:
    filter:
      namespace: my_namespace
```

In this way you can create a `Role` and don’t need to create a `ClusterRole`,
reducing the scope of the collector service account to a single namespace.

## Using Resource detector processor

Recently the OpenTelemetry operator incorporated a new feature introduced in
version v0.50.0, see
[PR #832](https://github.com/open-telemetry/opentelemetry-operator/pull/832).

This feature sets the `OTEL_RESOURCE_ATTRIBUTES` environment variable on the
collector container with the k8s pod attributes. It lets you to use the
resource detector processor, which attaches the environment variable values to
the spans. This only works when the collector is deployed in sidecar mode.

For example if you deploy this manifest:

```yaml
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: sidecar-for-my-app
spec:
  mode: sidecar
  image: ghcr.io/open-telemetry/opentelemetry-collector-releases/opentelemetry-collector-contrib:0.47.0
  config: |
    receivers:
      jaeger:
        protocols:
          grpc:
          thrift_binary:
          thrift_compact:
          thrift_http:
      otlp:
        protocols:
          grpc:
          http:

    processors:
      resourcedetection:
        detectors: [env]
        timeout: 2s
        override: false

    exporters:
      jaeger:
        endpoint: jaeger-all-in-one-collector:14250
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp, jaeger]
          processors: [resourcedetection]
          exporters: [jaeger]
```

And then deploy the vertx app example, you can see the `OTEL_RESOURCE_ATTRIBUTES`
environment variable was injected with some values in the sidecar container. Some of them use the Kubernetes downward API to get the attribute values.

An example of the value of the env var:

```yaml
- name: OTEL_RESOURCE_ATTRIBUTES
  value: k8s.deployment.name=dep-vertex,k8s.deployment.uid=ef3fe26b-a690-4746-9119-d2dbd94b469f,
  k8s.namespace.name=default,k8s.node.name=$(OTEL_RESOURCE_ATTRIBUTES_NODE_NAME),k8s.pod.name=
  (OTEL_RESOURCE_ATTRIBUTES_POD_NAME),k8s.pod.uid=$(OTEL_RESOURCE_ATTRIBUTES_POD_UID),k8s.replicaset
  name=dep-vertex-59b6f76585,k8s.replicaset.uid=5127bc38-e298-40e1-95df-f4a777e3176c
```

## Conclusion

We have learned about how to do a basic configuration that allows the
OpenTelemetry collector to attach Kubernetes resource attributes to traces.
There could be other complex scenarios but I hope this could serve as a base
that allows the reader to understand how the to add Kubernetes attributes to the
spans generated by the pods. We explored how to configure the k8sattributes
operator in order to accomplish this, and saw some improvements introduced on
the OpenTelemetry operator that allow us to do this in a simple way on sidecar
mode

## References

- [K8sattributes processor](https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/processor/k8sattributesprocessor)
- [K8sattributes processor RBAC](https://pkg.go.dev/github.com/open-telemetry/opentelemetry-collector-contrib/processor/k8sattributesprocessor#hdr-RBAC)
- [OpenTelemetry Kubernetes attributes](/docs/reference/specification/resource/semantic_conventions/k8s)
